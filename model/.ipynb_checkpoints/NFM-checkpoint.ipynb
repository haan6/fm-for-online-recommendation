{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "@author: Yeji Han\n",
    "\n",
    "A PyTorch Implementation of Online NFM with Hedge Backpropagation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from time import time\n",
    "from mab import algs\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-56c85dbfbed5>, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-56c85dbfbed5>\"\u001b[0;36m, line \u001b[0;32m82\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class NFM(torch.nn.Module):\n",
    "    def __init__(self, field_size, feature_sizes, max_num_hidden_layers, qtd_neuron_per_hidden_layer,\n",
    "                 dropout_shallow=[0.5], embedding_size=4, n_classes=2, batch_size=1,\n",
    "                 verbose=False, interaction_type=True, eval_metric=roc_auc_score,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda = True, greater_is_better = True):\n",
    "        super(NFM, self).__init__()\n",
    "\n",
    "        # Check CUDA\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA\")\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.field_size = field_size\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.dropout_shallow = dropout_shallow\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.interaction_type = interaction_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.use_cuda = use_cuda\n",
    "        self.greater_is_better = greater_is_better\n",
    "\n",
    "        self.b = Parameter(torch.tensor(b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(s), requires_grad=False).to(self.device)\n",
    "\n",
    "        # FM Part\n",
    "        print(\"Initializing FM\")\n",
    "        self.first_order_embeddings = nn.ModuleList([nn.Embedding(feature_size,1)\n",
    "                                                     for feature_size in self.feature_sizes])\n",
    "        if self.dropout_shallow:\n",
    "            self.first_order_dropout = nn.Dropout(self.dropout_shallow[0])\n",
    "        self.second_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, self.embedding_size)\n",
    "                                                      for feature_size in self.feature_sizes])\n",
    "        self.bias = torch.nn.Parameter(torch.randn(1))\n",
    "        print(\"Initializing FM Done\")\n",
    "\n",
    "        # Neural Networks Part\n",
    "        print(\"Initializing Neural Networks\")\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        if self.interaction_type:\n",
    "            self.hidden_layers.append(nn.Linear(embedding_size, qtd_neuron_per_hidden_layer))\n",
    "        else:\n",
    "            self.hidden_layers.append(nn.Linear(self.field_size * (self.field_size-1) / 2, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "\n",
    "        print(\"Initializing Neural Networks Done\")\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "    \n",
    "        def forward(self, Xi, Xv):\n",
    "        \"\"\"\n",
    "        :param Xi: index input tensor, batch_size * k * 1\n",
    "        :param Xv: value input tensor, batch_size * k * 1\n",
    "        :return: the last output\n",
    "        \"\"\"\n",
    "\n",
    "        # FM Part\n",
    "        for i, emb in enumerate(self.first_order_embeddings):\n",
    "            print(i)\n",
    "        first_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                               for i, emb in enumerate(self.first_order_embeddings)]\n",
    "        first_order = torch.cat(first_order_emb_arr, 1)\n",
    "        if self.dropout_shallow:\n",
    "            first_order = self.first_order_dropout(first_order)\n",
    "\n",
    "        if self.interaction_type:\n",
    "            # Use 2xixj = (xi+xj)^2 - xi^2 - yj^2 to reduce calculation\n",
    "            second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                                    for i, emb in enumerate(self.second_order_embeddings)]\n",
    "            sum_second_order_emb = sum(second_order_emb_arr)\n",
    "            # (xi+xj)^2\n",
    "            sum_second_order_emb_square = sum_second_order_emb * sum_second_order_emb\n",
    "            # xi^2+xj^2\n",
    "            second_order_emb_square = [item * item for item in second_order_emb_arr]\n",
    "            second_order_emb_square_sum = sum(second_order_emb_square)\n",
    "            second_order = (sum_second_order_emb_square - second_order_emb_square_sum) * 0.5\n",
    "        else:\n",
    "            second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                                    for i, emb in enumerate(self.second_order_embeddings)]\n",
    "            weights_fm = []\n",
    "            for i in range(self.field_size):\n",
    "                for j in range(i + 1, self.field_size):\n",
    "                    weights_fm.append(second_order_emb_arr[i] * second_order_emb_arr[j])\n",
    "\n",
    "        # Neural Networks Part\n",
    "        if self.interation_type:\n",
    "            x = second_order\n",
    "        else:\n",
    "            x = torch.cat([torch.sum(weight_fm, 1).view([-1, 1])\n",
    "                            for weight_fm in weights_fm], 1)\n",
    "        hidden_connections = []\n",
    "        activation = F.relu\n",
    "\n",
    "        x = activation(self.hidden_layers[0](x))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "\n",
    "        return first_order, pred_per_layer\n",
    "\n",
    "    def update_weights(self, Xi, Xv, Y, show_loss):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        first_order, predictions_per_layer = self.forward(Xi, Xv)\n",
    "\n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes),\n",
    "                             Y.view(self.batch_size).long())\n",
    "            losses_per_layer.append(loss)\n",
    "\n",
    "        w = []\n",
    "        b = []\n",
    "\n",
    "        for i in range(len(losses_per_layer)):\n",
    "            losses_per_layer[i].backward(retain_graph=True)\n",
    "            self.output_layers[i].weight.data -= self.n * \\\n",
    "                self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "            self.output_layers[i].bias.data -= self.n * \\\n",
    "                self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "            w.append(self.alpha[i] * self.hidden_layers[i].weight.grad.data)\n",
    "            b.append(self.alpha[i] * self.hidden_layers[i].bias.grad.data)\n",
    "            self.zero_grad()\n",
    "\n",
    "        for i in range(1, len(losses_per_layer)):\n",
    "            self.hidden_layers[i].weight.data -= self.n * torch.sum(torch.cat(w[i:]))\n",
    "            self.hidden_layers[i].bias.data -= self.n * torch.sum(torch.cat(b[i:]))\n",
    "\n",
    "        for i in range(len(losses_per_layer)):\n",
    "            self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "            self.alpha[i] = torch.max(self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "\n",
    "        if show_loss:\n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def partial_fit_(self, Xi_data, Xv_data, Y_data, show_loss=True):\n",
    "        self.update_weights(Xi_data, Xv_data, Y_data, show_loss)\n",
    "\n",
    "    def partial_fit(self, Xi_data, Xv_data, Y_data, show_loss=True):\n",
    "        self.partial_fit_(Xi_data, Xv_data, Y_data, show_loss)\n",
    "\n",
    "    def predict_(self, Xi_data, Xv_data):\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(Xi_data)).view(\n",
    "                self.max_num_hidden_layers, len(Xi_data), 1), self.forward(Xi_data, Xv_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, Xi_data, Xv_data):\n",
    "        pred = self.predict_(Xi_data, Xv_data)\n",
    "        return pred\n",
    "\n",
    "    def plot_accuracy(self, Xi_data, Xv_data, Y_data):\n",
    "        right_pred = 0\n",
    "        result = []\n",
    "\n",
    "        for i in range(len(Y_data)):\n",
    "            pred = self.predict(Xi_data, Xv_data)\n",
    "            if (Y_data[i] == pred):\n",
    "                right_pred += 1\n",
    "\n",
    "            result.append(right_pred / result * 100)\n",
    "\n",
    "        plt.plot([i for i in range(len(result))], result)\n",
    "        plt.xlabel('Number of data')\n",
    "        plt.ylabel('Right Predictions / Whole Predictions * 100 (%)')\n",
    "\n",
    "        plt.grid()\n",
    "        plt.title('Accuracy')\n",
    "\n",
    "        plt.savefig('Accuracy.png')\n",
    "        plt.show()\n",
    "\n",
    "    def export_params_to_json(self):\n",
    "        state_dict = self.state_dict()\n",
    "        params_gp = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            params_gp[key] = tensor.cpu().numpy().tolist()\n",
    "\n",
    "        return json.dumps(params_gp)\n",
    "\n",
    "    def load_params_from_json(self, json_data):\n",
    "        params = json.loads(json_data)\n",
    "        o_dict = collections.OrderedDict()\n",
    "        for key, tensor in params.items():\n",
    "            o_dict[key] = torch.tensor(tensor).to(self.device)\n",
    "        self.load_state_dict(o_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'data_preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b79bf5004621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresult_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_criteo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/tiny_train_input.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../data/category_emb.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'data_preprocess'"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import data_preprocess\n",
    "\n",
    "result_dict = data_preprocess.read_criteo_data('../data/tiny_train_input.csv', '../data/category_emb.csv')\n",
    "test_dict = data_preprocess.read_criteo_data('../data/tiny_test_input.csv', '../data/category_emb.csv')\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    nfm = NFM(39, result_dict['feature_sizes'], max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=10, verbose=True, use_cuda=True, interaction_type=True).cuda()\n",
    "    nfm.plot_accuracy(result_dict['index'], result_dict['value'], result_dict['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
