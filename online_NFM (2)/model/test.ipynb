{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['/home/yeji/Myenv/lib/python36.zip', '/home/yeji/Myenv/lib/python3.6', '/home/yeji/Myenv/lib/python3.6/lib-dynload', '/usr/lib/python3.6', '', '/home/yeji/Myenv/local/lib/python3.6/site-packages', '/home/yeji/Myenv/lib/python3.6/site-packages', '/home/yeji/Myenv/local/lib/python3.6/site-packages/IPython/extensions', '/home/yeji/.ipython', '../', './model', '../', './model', '../', './model', '../', './model', '../', '../', '../', '../', '../', '../', '../', '../', '.', '.', '.']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "@author: Yeji Han\n",
    "\n",
    "This script is used to preprocess the raw data file\n",
    "\"\"\"\n",
    "\n",
    "def load_criteo_category_index(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    cate_dict = []\n",
    "\n",
    "    for i in range(39):\n",
    "        cate_dict.append({})\n",
    "\n",
    "    for line in f:\n",
    "        data = line.strip().split(',')\n",
    "        cate_dict[int(data[0])][data[1]] = int(data[2])\n",
    "\n",
    "    return cate_dict\n",
    "\n",
    "def read_criteo_data(file_path, emb_file):\n",
    "    result = {'size': 0, 'label': [], 'index': [], 'value': [], 'feature_sizes':[]}\n",
    "    cate_dict = load_criteo_category_index(emb_file)\n",
    "\n",
    "    for item in cate_dict:\n",
    "        result['feature_sizes'].append(len(item))\n",
    "\n",
    "    f = open(file_path, 'r')\n",
    "    for line in f:\n",
    "        data = line.strip().split(',')\n",
    "        result['label'].append(int(data[0]))\n",
    "        indices = [int(item) for item in data[1:]]\n",
    "        values = [1 for i in range(39)]\n",
    "        result['index'].append(indices)\n",
    "        result['value'].append(values)\n",
    "\n",
    "    result['size'] += len(result['value'])\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "@author: Yeji Han\n",
    "\n",
    "A PyTorch implementation of Online FM\n",
    "\"\"\"\n",
    "\n",
    "from time import time\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch.backends.cudnn\n",
    "\n",
    "class FM(torch.nn.Module):\n",
    "    def __init__(self, field_size, feature_sizes, embedding_size=4, is_shallow_dropout=True,\n",
    "                 dropout_shallow=[0.5], n_epochs=64, batch_size=256, interaction_type=True,\n",
    "                 verbose=False, random_seed=990211, weight_decay=0.0, loss_type='logloss',\n",
    "                 b=0.99, n=0.01, eval_metric=roc_auc_score, use_cuda=True, greater_is_better=True):\n",
    "        super(FM, self).__init__()\n",
    "\n",
    "        # Check CUDA\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA\")\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.field_size = field_size\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.is_shallow_dropout = is_shallow_dropout\n",
    "        self.dropout_shallow = dropout_shallow\n",
    "        self.batch_size = batch_size\n",
    "        self.interaction_type = interaction_type\n",
    "        self.n = n\n",
    "        self.verbose = verbose\n",
    "        self.weight_decay = weight_decay\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.use_cuda = use_cuda\n",
    "        self.greater_is_better = greater_is_better\n",
    "\n",
    "        self.b = Parameter(torch.tensor(b), requires_grad=False).to(self.device)\n",
    "\n",
    "        # FM\n",
    "        self.first_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, 1)\n",
    "                                                     for feature_size in self.feature_sizes]).to(self.device)\n",
    "        if self.dropout_shallow:\n",
    "            self.first_order_dropout = nn.Dropout(self.dropout_shallow[0]).to(self.device)\n",
    "\n",
    "        self.second_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, self.embedding_size)\n",
    "                                                      for feature_size in self.feature_sizes]).to(self.device)\n",
    "\n",
    "    def forward(self, Xi, Xv):\n",
    "        first_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                               for i, emb in enumerate(self.first_order_embeddings)]\n",
    "        first_order = torch.cat(first_order_emb_arr, 1)\n",
    "\n",
    "        if self.dropout_shallow:\n",
    "            first_order = self.first_order_dropout(first_order)\n",
    "\n",
    "        if self.interaction_type:\n",
    "            # Use 2xixj = (xi+xj)^2 - xi^2 - yj^2 to reduce calculation\n",
    "            second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                                    for i, emb in enumerate(self.second_order_embeddings)]\n",
    "            sum_second_order_emb = sum(second_order_emb_arr)\n",
    "            # (xi+xj)^2\n",
    "            sum_second_order_emb_square = sum_second_order_emb * sum_second_order_emb\n",
    "            # xi^2+xj^2\n",
    "            second_order_emb_square = [item * item for item in second_order_emb_arr]\n",
    "            second_order_emb_square_sum = sum(second_order_emb_square)\n",
    "            second_order = (sum_second_order_emb_square - second_order_emb_square_sum) * 0.5\n",
    "\n",
    "        else:\n",
    "            second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                                    for i, emb in enumerate(self.second_order_embeddings)]\n",
    "            weights_fm = []\n",
    "            for i in range(self.field_size):\n",
    "                for j in range(i + 1, self.field_size):\n",
    "                    weights_fm.append(second_order_emb_arr[i] * second_order_emb_arr[j])\n",
    "\n",
    "        total_sum = self.b + torch.sum(first_order, 1) + torch.sum(second_order, 1)\n",
    "        return total_sum\n",
    "\n",
    "    def fit(self, Xi, Xv, Y):\n",
    "        Xi = Variable(torch.LongTensor(Xi).reshape(-1, self.field_size, 1)).to(self.device)\n",
    "        Xv = Variable(torch.FloatTensor(Xv).reshape(-1, self.field_size)).to(self.device)\n",
    "        Y = Variable(torch.FloatTensor(Y)).to(self.device)\n",
    "\n",
    "        model = self.train()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.n)\n",
    "        criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xi, Xv)\n",
    "        loss = criterion(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def predict(self, Xi_data, Xv_data):\n",
    "        Xi = Variable(torch.LongTensor(Xi_data).reshape(-1, self.field_size, 1)).to(self.device)\n",
    "        Xv = Variable(torch.FloatTensor(Xv_data).reshape(-1, self.field_size)).to(self.device)\n",
    "        model = self.eval()\n",
    "        output = model(Xi, Xv)\n",
    "        pred = torch.sigmoid(output).cpu()\n",
    "\n",
    "        return pred.data.numpy() > 0.5\n",
    "\n",
    "    def roc_score(self, pred, train_Y):\n",
    "        confusion_matrix = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "        for j in range(pred.shape[0]):\n",
    "            if pred[j] == train_Y[j]:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"tp\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"tn\"] += 1\n",
    "            else:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"fn\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"fp\"] += 1\n",
    "\n",
    "        if confusion_matrix['tp'] + confusion_matrix['fp'] != 0:\n",
    "            tpr = confusion_matrix['tp'] / (confusion_matrix['tp'] + confusion_matrix['fp'])\n",
    "        else:\n",
    "            tpr = 0\n",
    "\n",
    "        if confusion_matrix['fp'] + confusion_matrix['tn'] != 0:\n",
    "            fpr = confusion_matrix['fp'] / (confusion_matrix['fp'] + confusion_matrix['tn'])\n",
    "        else:\n",
    "            fpr = 0\n",
    "\n",
    "        return {\"tpr\": tpr, \"fpr\": fpr}\n",
    "\n",
    "    def evaluate(self, train_Xi, train_Xv, train_Y):\n",
    "        train_size = len(train_Y)\n",
    "        time_elapsed = 0\n",
    "        roc = []\n",
    "\n",
    "        start = time()\n",
    "        for i in range(train_size):\n",
    "            end = i + self.batch_size\n",
    "\n",
    "            if end < train_size:\n",
    "                self.fit(train_Xi[i:end], train_Xv[i:end], train_Y[i:end])\n",
    "            else:\n",
    "                self.fit(train_Xi[i:train_size], train_Xv[i:train_size], train_Y[i:train_size])\n",
    "\n",
    "            pred = self.predict(train_Xi, train_Xv)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                roc.append(self.roc_score(pred, train_Y))\n",
    "\n",
    "        time_elapsed = time() - start\n",
    "        return time_elapsed, roc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "@author: Yeji Han\n",
    "\n",
    "A PyTorch Implementation of Online NFM with Hedge Backpropagation\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch.backends.cudnn\n",
    "\n",
    "class ONN_NFM(torch.nn.Module):\n",
    "    def __init__(self, field_size, feature_sizes, max_num_hidden_layers, qtd_neuron_per_hidden_layer,\n",
    "                 dropout_shallow=[0.5], embedding_size=4, n_classes=2, batch_size=1,\n",
    "                 verbose=False, interaction_type=True, eval_metric=roc_auc_score,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=True, greater_is_better=True):\n",
    "        super(ONN_NFM, self).__init__()\n",
    "\n",
    "        # Check CUDA\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA\")\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.field_size = field_size\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.dropout_shallow = dropout_shallow\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.interaction_type = interaction_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.use_cuda = use_cuda\n",
    "        self.greater_is_better = greater_is_better\n",
    "\n",
    "        self.b = Parameter(torch.tensor(b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(s), requires_grad=False).to(self.device)\n",
    "\n",
    "        # FM Part\n",
    "        print(\"Initializing FM\")\n",
    "        self.first_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, 1)\n",
    "                                                     for feature_size in self.feature_sizes]).to(self.device)\n",
    "        if self.dropout_shallow:\n",
    "            self.first_order_dropout = nn.Dropout(self.dropout_shallow[0]).to(self.device)\n",
    "        self.second_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, self.embedding_size)\n",
    "                                                      for feature_size in self.feature_sizes]).to(self.device)\n",
    "        self.bias = Parameter(torch.randn(1)).to(self.device)\n",
    "        print(\"Initializing FM Done\")\n",
    "\n",
    "        # Neural Networks Part\n",
    "        print(\"Initializing Neural Networks\")\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        if self.interaction_type:\n",
    "            self.hidden_layers.append(nn.Linear(embedding_size, qtd_neuron_per_hidden_layer))\n",
    "        else:\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(self.field_size * (self.field_size - 1) / 2, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "\n",
    "        print(\"Initializing Neural Networks Done\")\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "\n",
    "    def second_order(self, Xi, Xv):\n",
    "        # FM Part\n",
    "        Xi = torch.LongTensor(Xi).to(self.device).reshape(-1, self.field_size, 1)\n",
    "        Xv = torch.FloatTensor(Xv).to(self.device).reshape(-1, self.field_size)\n",
    "        second_order_emb_arr = [torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i].t()\n",
    "                                for i, emb in enumerate(self.second_order_embeddings)]\n",
    "        sum_second_order_emb = sum(second_order_emb_arr)\n",
    "        # (xi+xj)^2\n",
    "        sum_second_order_emb_square = sum_second_order_emb * sum_second_order_emb\n",
    "        # xi^2+xj^2\n",
    "        second_order_emb_square = [item * item for item in second_order_emb_arr]\n",
    "        second_order_emb_square_sum = sum(second_order_emb_square)\n",
    "        second_order = (sum_second_order_emb_square - second_order_emb_square_sum) * 0.5\n",
    "\n",
    "        return second_order.t()\n",
    "\n",
    "    def forward(self, Xi, Xv):\n",
    "        # Neural Networks Part\n",
    "        x = self.second_order(Xi, Xv)\n",
    "\n",
    "        hidden_connections = []\n",
    "        activation = F.relu\n",
    "\n",
    "        x = activation(self.hidden_layers[0](x))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "        return pred_per_layer\n",
    "\n",
    "    def update_weights(self, Xi, Xv, Y, show_loss):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        predictions_per_layer = self.forward(Xi, Xv)\n",
    "\n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes),\n",
    "                             Y.view(self.batch_size).long())\n",
    "            losses_per_layer.append(loss)\n",
    "\n",
    "        w = []\n",
    "        b = []\n",
    "\n",
    "        for i in range(len(losses_per_layer)):\n",
    "            losses_per_layer[i].backward(retain_graph=True)\n",
    "            self.output_layers[i].weight.data -= self.n * \\\n",
    "                                                 self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "            self.output_layers[i].bias.data -= self.n * \\\n",
    "                                               self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "            w.append(self.alpha[i] * self.hidden_layers[i].weight.grad.data)\n",
    "            b.append(self.alpha[i] * self.hidden_layers[i].bias.grad.data)\n",
    "            self.zero_grad()\n",
    "\n",
    "        for i in range(1, len(losses_per_layer)):\n",
    "            self.hidden_layers[i].weight.data -= self.n * torch.sum(torch.cat(w[i:]))\n",
    "            self.hidden_layers[i].bias.data -= self.n * torch.sum(torch.cat(b[i:]))\n",
    "\n",
    "        for i in range(len(losses_per_layer)):\n",
    "            self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "            self.alpha[i] = torch.max(self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "\n",
    "        if show_loss:\n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def partial_fit_(self, Xi_data, Xv_data, Y_data, show_loss=False):\n",
    "        self.update_weights(Xi_data, Xv_data, Y_data, show_loss)\n",
    "\n",
    "    def partial_fit(self, Xi_data, Xv_data, Y_data, show_loss=False):\n",
    "        self.partial_fit_(Xi_data, Xv_data, Y_data, show_loss)\n",
    "\n",
    "    def predict_(self, Xi_data, Xv_data):\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(Xi_data)).view(\n",
    "                self.max_num_hidden_layers, len(Xi_data), 1), self.forward(Xi_data, Xv_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, Xi_data, Xv_data):\n",
    "        pred = self.predict_(Xi_data, Xv_data)\n",
    "        return pred\n",
    "\n",
    "    def roc_score(self, pred, train_Y):\n",
    "        confusion_matrix = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "        for j in range(pred.shape[0]):\n",
    "            if pred[j] == train_Y[j]:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"tp\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"tn\"] += 1\n",
    "            else:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"fn\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"fp\"] += 1\n",
    "\n",
    "        if confusion_matrix['tp'] + confusion_matrix['fp'] != 0:\n",
    "            tpr = confusion_matrix['tp'] / (confusion_matrix['tp'] + confusion_matrix['fp'])\n",
    "        else:\n",
    "            tpr= 0\n",
    "\n",
    "        if confusion_matrix['fp'] + confusion_matrix['tn'] != 0:\n",
    "            fpr = confusion_matrix['fp'] / (confusion_matrix['fp'] + confusion_matrix['tn'])\n",
    "        else:\n",
    "            fpr = 0\n",
    "\n",
    "        return {\"tpr\": tpr, \"fpr\": fpr}\n",
    "\n",
    "    def evaluate(self, train_Xi, train_Xv, train_Y):\n",
    "        roc = []\n",
    "        time_elapsed = 0\n",
    "\n",
    "        start = time()\n",
    "        for i in range(len(train_Y)):\n",
    "            self.partial_fit(np.array(train_Xi[i]), np.array(train_Xv[i]), np.array(train_Y[i]))\n",
    "            pred = self.predict(train_Xi, train_Xv)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                roc.append(self.roc_score(pred, train_Y))\n",
    "\n",
    "        time_elapsed = time() - start\n",
    "        return time_elapsed, roc\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "@author: Yeji Han\n",
    "\n",
    "A PyTorch implementation of Online NFM with SGD\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.backends.cudnn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class SGD_NFM(torch.nn.Module):\n",
    "    def __init__(self, field_size, feature_sizes, max_num_hidden_layers, qtd_neuron_per_hidden_layer,\n",
    "                 dropout_shallow=[0.5], embedding_size=4, n_classes=2, n_epochs=64, batch_size=100,\n",
    "                 loss_type='logloss', verbose=False, interaction_type=True, eval_metric=accuracy_score,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=True, greater_is_better=True):\n",
    "\n",
    "        super(SGD_NFM, self).__init__()\n",
    "\n",
    "        # Check CUDA\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA\")\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.field_size = field_size\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.dropout_shallow = dropout_shallow\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_type = loss_type\n",
    "        self.verbose = verbose\n",
    "        self.interaction_type = interaction_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.use_cuda = use_cuda\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.n = n\n",
    "\n",
    "        self.b = Parameter(torch.tensor(b), requires_grad=False).to(self.device)\n",
    "\n",
    "        # FM part\n",
    "        self.first_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, 1)\n",
    "                                                     for feature_size in self.feature_sizes]).to(self.device)\n",
    "        if self.dropout_shallow:\n",
    "            self.first_order_dropout = nn.Dropout(self.dropout_shallow[0]).to(self.device)\n",
    "\n",
    "        self.second_order_embeddings = nn.ModuleList([nn.Embedding(feature_size, self.embedding_size)\n",
    "                                                      for feature_size in self.feature_sizes]).to(self.device)\n",
    "\n",
    "        # Neural Networks\n",
    "        self.hidden_layers = []\n",
    "\n",
    "        if self.interaction_type:\n",
    "            self.hidden_layers.append(nn.Linear(embedding_size, qtd_neuron_per_hidden_layer))\n",
    "        else:\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(self.field_size * (self.field_size - 1) / 2, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "\n",
    "    def forward(self, Xi, Xv):\n",
    "        # FM\n",
    "        first_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                               for i, emb in enumerate(self.first_order_embeddings)]\n",
    "        first_order = torch.cat(first_order_emb_arr, 1)\n",
    "\n",
    "        if self.dropout_shallow:\n",
    "            first_order = self.first_order_dropout(first_order)\n",
    "\n",
    "        if self.interaction_type:\n",
    "            # Use 2xixj = (xi+xj)^2 - xi^2 - yj^2 to reduce calculation\n",
    "            second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                                    for i, emb in enumerate(self.second_order_embeddings)]\n",
    "            sum_second_order_emb = sum(second_order_emb_arr)\n",
    "            # (xi+xj)^2\n",
    "            sum_second_order_emb_square = sum_second_order_emb * sum_second_order_emb\n",
    "            # xi^2+xj^2\n",
    "            second_order_emb_square = [item * item for item in second_order_emb_arr]\n",
    "            second_order_emb_square_sum = sum(second_order_emb_square)\n",
    "            second_order = (sum_second_order_emb_square - second_order_emb_square_sum) * 0.5\n",
    "\n",
    "        else:\n",
    "            second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t()\n",
    "                                    for i, emb in enumerate(self.second_order_embeddings)]\n",
    "            weights_fm = []\n",
    "            for i in range(self.field_size):\n",
    "                for j in range(i + 1, self.field_size):\n",
    "                    weights_fm.append(second_order_emb_arr[i] * second_order_emb_arr[j])\n",
    "\n",
    "        # Neural Networks\n",
    "        if self.interaction_type:\n",
    "            x = second_order\n",
    "        else:\n",
    "            x = torch.cat([torch.sum(weight_fm, 1).view([-1, 1])\n",
    "                           for weight_fm in weights_fm], 1)\n",
    "\n",
    "        activation = F.relu\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            x = activation(self.hidden_layers[i](x))\n",
    "\n",
    "        # Sum\n",
    "        total_sum = self.b + torch.sum(first_order, 1) + torch.sum(x, 1)\n",
    "        return total_sum\n",
    "\n",
    "    def fit(self, Xi, Xv, Y):\n",
    "        Xi = Variable(torch.LongTensor(Xi).reshape(-1, self.field_size, 1)).to(self.device)\n",
    "        Xv = Variable(torch.FloatTensor(Xv).reshape(-1, self.field_size)).to(self.device)\n",
    "        Y = Variable(torch.FloatTensor(Y)).to(self.device)\n",
    "\n",
    "        model = self.train()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.n)\n",
    "        criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        epoch_begin_time = time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xi, Xv)\n",
    "        loss = criterion(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def predict(self, Xi_data, Xv_data):\n",
    "        Xi = Variable(torch.LongTensor(Xi_data).reshape(-1, self.field_size, 1)).to(self.device)\n",
    "        Xv = Variable(torch.FloatTensor(Xv_data).reshape(-1, self.field_size)).to(self.device)\n",
    "\n",
    "        model = self.eval()\n",
    "        output = model(Xi, Xv)\n",
    "        pred = torch.sigmoid(output).cpu()\n",
    "        return pred.data.numpy() > 0.5\n",
    "\n",
    "    def roc_score(pred, train_Y):\n",
    "        confusion_matrix = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "        for j in range(pred):\n",
    "            if pred[j] == train_Y[j]:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"tp\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"tn\"] += 1\n",
    "            else:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"fn\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"fp\"] += 1\n",
    "\n",
    "        tpr = confusion_matrix['tp'] / (confusion_matrix['tp'] + confusion_matrix['fp'])\n",
    "        fpr = confusion_matrix['fp'] / (confusion_matrix['fp'] + confusion_matrix['tn'])\n",
    "\n",
    "        return {\"tpr\": tpr, \"fpr\": fpr}\n",
    "\n",
    "    def roc_score(self, pred, train_Y):\n",
    "        confusion_matrix = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "\n",
    "        for j in range(pred.shape[0]):\n",
    "            if pred[j] == train_Y[j]:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"tp\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"tn\"] += 1\n",
    "            else:\n",
    "                if train_Y[j] == 1:\n",
    "                    confusion_matrix[\"fn\"] += 1\n",
    "                else:\n",
    "                    confusion_matrix[\"fp\"] += 1\n",
    "\n",
    "        if confusion_matrix['tp'] + confusion_matrix['fp'] != 0:\n",
    "            tpr = confusion_matrix['tp'] / (confusion_matrix['tp'] + confusion_matrix['fp'])\n",
    "        else:\n",
    "            tpr = 0\n",
    "\n",
    "        if confusion_matrix['fp'] + confusion_matrix['tn'] != 0:\n",
    "            fpr = confusion_matrix['fp'] / (confusion_matrix['fp'] + confusion_matrix['tn'])\n",
    "        else:\n",
    "            fpr = 0\n",
    "\n",
    "        return {\"tpr\": tpr, \"fpr\": fpr}\n",
    "\n",
    "    def evaluate(self, train_Xi, train_Xv, train_Y):\n",
    "        train_size = len(train_Y)\n",
    "        time_elapsed = 0\n",
    "        confusion_matrix = {\n",
    "            \"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0\n",
    "        }\n",
    "        roc = []\n",
    "\n",
    "        start = time()\n",
    "        for i in range(train_size):\n",
    "            end = i + self.batch_size\n",
    "            if end < train_size:\n",
    "                self.fit(train_Xi[i:end], train_Xv[i:end], train_Y[i:end])\n",
    "            else:\n",
    "                self.fit(train_Xi[i:train_size], train_Xv[i:train_size], train_Y[i:train_size])\n",
    "\n",
    "            pred = self.predict(train_Xi, train_Xv)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                roc.append(self.roc_score(pred, train_Y))\n",
    "\n",
    "\n",
    "        time_elapsed = time() - start\n",
    "        return time_elapsed, roc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "===== Importing Dataset =====\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7c60c3cfeba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===== Importing Dataset =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_criteo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/tiny_train_input.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/category_emb.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_dict_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-2b71d8d443d2>\u001b[0m in \u001b[0;36mread_criteo_data\u001b[0;34m(file_path, emb_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_criteo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature_sizes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mcate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_criteo_category_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-2b71d8d443d2>\u001b[0m in \u001b[0;36mload_criteo_category_index\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_criteo_category_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/category_emb.csv'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/category_emb.csv'",
     "output_type": "error"
    }
   ],
   "source": [
    "print(\"===== Importing Dataset =====\")\n",
    "\n",
    "train_dict = read_criteo_data('tiny_train_input.csv', 'category_emb.csv')\n",
    "train_dict_size = train_dict['size']\n",
    "\n",
    "train_Xi, train_Xv, train_Y \\\n",
    "    = train_dict['index'][:int(train_dict_size * 0.05)], \\\n",
    "      train_dict['value'][:int(train_dict_size * 0.05)], \\\n",
    "      train_dict['label'][:int(train_dict_size * 0.05)]\n",
    "\n",
    "print(\"===== Dataset is Ready =====\")\n",
    "\n",
    "# with torch.cuda.device(0):\n",
    "time_elapsed = {\"FM\": 0, \"SGD_NFM\": 0, \"ONN_NFM\": 0}\n",
    "roc_scores = {\"FM\": [], \"SGD_NFM\": [], \"ONN_NFM\": []}\n",
    "\n",
    "print(\"===== Instantiating Models =====\")\n",
    "\n",
    "fm = FM(39, train_dict['feature_sizes'], batch_size=20)\n",
    "sgd_nfm = SGD_NFM(39, train_dict['feature_sizes'], 5, 10, batch_size=20)\n",
    "onn_nfm = ONN_NFM(39, train_dict['feature_sizes'], max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=10,\n",
    "                  verbose=True, use_cuda=True, interaction_type=True)\n",
    "\n",
    "models = [(fm, \"FM\"), (sgd_nfm, \"SGD_NFM\"), (onn_nfm, \"ONN_NFM\")]\n",
    "\n",
    "print(\"===== Models are Ready =====\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "===== Training FM =====\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c6de97af0443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"===== Training FM =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Xi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Xv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"===== Evaluating FM is Finished. Time: {time_elapsed[models[0][1]]} =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "print(f\"===== Training FM =====\")\n",
    "time_elapsed[models[0][1]], roc_scores[models[0][1]] = models[0][0].evaluate(train_Xi, train_Xv, train_Y)\n",
    "print(f\"===== Evaluating FM is Finished. Time: {time_elapsed[models[0][1]]} =====\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"===== Training SGD_NFM =====\")\n",
    "time_elapsed[models[1][1]], roc_scores[models[1][1]] = models[1][0].evaluate(train_Xi, train_Xv, train_Y)\n",
    "print(f\"===== Evaluating SGD_NFM is Finished. Time: {time_elapsed[models[1][1]]} =====\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"===== Training ONN_NFM =====\")\n",
    "time_elapsed[models[2][1]], roc_scores[models[2][1]] = models[2][0].evaluate(train_Xi, train_Xv, train_Y)\n",
    "print(f\"===== Evaluating ONN_NFM is Finished. Time: {time_elapsed[models[2][1]]} =====\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "===== Drawing a plot =====\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fc827d713c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FPR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TPR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'ylim'"
     ],
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'ylim'",
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"===== Drawing a plot =====\")\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime('%Y-%m-%d')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.ylim(-0.04, 1.04)\n",
    "ax.xlabel('FPR')\n",
    "ax.ylabel('TPR')\n",
    "\n",
    "for i, (mark, color) in enumerate(zip(\n",
    "        ['s', 'o', 'v'], ['r', 'g', 'b'])):\n",
    "    ax.plot(roc_scores[i]['fpr'], roc_scores[i]['tpr'], color=color,\n",
    "            marker=mark,\n",
    "            markerfacecolor='None',\n",
    "            markeredgecolor=color,\n",
    "            linestyle='None',\n",
    "            label=models[i][1])\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.title(f'ROC Score (Dataset #: {len(train_Y)}')\n",
    "ax.legend(loc='lower right')\n",
    "fig.savefig(f'{date}_roc_score.png')\n",
    "\n",
    "print(\"===== Drawing Plot is Finished=====\")\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}