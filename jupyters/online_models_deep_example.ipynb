{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Deep Factorization Machine\n",
    "Online factorization models take single data as an input, make a prediction, and train with the data. \n",
    "This notebook demonstrates fitting online models with an adam optimizer and those with hedge backpropagation (HBP) to criteo data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "The from models imports the package for use. We have also imported a few other packages for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "\n",
    "\n",
    "from utils import data_preprocess, plot\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from time import time\n",
    "\n",
    "from models.models_online_deep.deepfm_adam import DeepFMAdam\n",
    "from models.models_online_deep.deepfm_onn import DeepFMOnn\n",
    "from models.models_online_deep.nfm_adam import NFMAdam\n",
    "from models.models_online_deep.nfm_onn import NFMOnn\n",
    "from models.models_online_deep.fm_adam import FMAdam\n",
    "\n",
    "from models.models_online_deep.afm_adam import AFMAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log = os.getcwd() + '../performance/save_log/'\n",
    "save_model = os.getcwd() + '../performance/save_model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a criteo dataset\n",
    "A dataset for a factorization machine requires indices, values, and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "train_dict = data_preprocess.read_criteo_data('../dataset/criteo/tiny_train_input.csv', '../dataset/criteo/category_emb.csv')\n",
=======
    "dataset_path = './../dataset/criteo/'\n",
    "dataset_input_path = dataset_path + 'tiny_train_input.csv'\n",
    "dataset_emb_path = dataset_path + 'tiny_train_input.csv'\n",
    "\n",
    "\n",
    "train_dict = data_preprocess.read_criteo_data(dataset_input_path, dataset_emb_path)\n",
>>>>>>> 23a9a3c58fbe44065acdcd99af45460f16c6bc81
    "train_dict_size = train_dict['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './../dataset/criteo/'\n",
    "dataset_input = dataset_path + 'tiny_train_input.csv'\n",
    "dataset_emb = dataset_path + 'tiny_train_input.csv'\n",
    "\n",
    "num_batchdata = 2500\n",
    "num_batch = 10\n",
    "data_config = 7\n",
    "# data_config = 3\n",
    "\n",
    "if data_config == \"Iteration\":\n",
    "    batch_train_Xi_list, batch_train_Xv_list, batch_train_Y_list, ratio_list \\\n",
<<<<<<< HEAD
    "        = data_preprocess.create_ten_iter('../dataset/criteo/tiny_train_input.csv', '../dataset/criteo/category_emb.csv', num_batch, num_batchdata)\n",
    "\n",
    "elif isinstance(data_config, int):\n",
    "    batch_train_Xi_list, batch_train_Xv_list, batch_train_Y_list, ratio_list \\\n",
    "        = data_preprocess.create_dataset('../dataset/criteo/tiny_train_input.csv', '../dataset/criteo/category_emb.csv', data_config, num_batch, num_batchdata)\n",
    "\n",
    "else:\n",
    "    batch_train_Xi_list, batch_train_Xv_list, batch_train_Y_list, ratio_list \\\n",
    "        = data_preprocess.create_dataset('../dataset/criteo/tiny_train_input.csv', '../dataset/criteo/category_emb.csv', int(num_batch/2), num_batch, num_batchdata)"
=======
    "        = data_preprocess.create_ten_iter(dataset_input_path , dataset_emb_path, num_batch, num_batchdata)\n",
    "\n",
    "elif isinstance(data_config, int):\n",
    "    batch_train_Xi_list, batch_train_Xv_list, batch_train_Y_list, ratio_list \\\n",
    "        = data_preprocess.create_dataset(dataset_input_path , dataset_emb_path, data_config, num_batch, num_batchdata)\n",
    "\n",
    "else:\n",
    "    batch_train_Xi_list, batch_train_Xv_list, batch_train_Y_list, ratio_list \\\n",
    "        = data_preprocess.create_dataset(dataset_input_path , dataset_emb_path, int(num_batch/2), num_batch, num_batchdata)"
>>>>>>> 23a9a3c58fbe44065acdcd99af45460f16c6bc81
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 5\n",
    "neuron_per_hidden_layer = 10\n",
    "data_feature_dim = 39\n",
    "embedding_size = 10\n",
    "n = 0.0001\n",
    "\n",
    "feature_sizes = [63, 113, 126, 51, 224, 148, 100, 79, 104, 9, 32, 57, 82, 1457, 555, 176373, 129683, 305, 19, 11887,\n",
    "                 632, 3, 41738, 5170, 175446, 3170, 27, 11356, 165602, 10, 4641, 2030, 4, 172761, 18, 15, 57903, 86,\n",
    "                 44549]\n",
    "# num_feature = sum(feature_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create deep factorization machines and online deep factorization machines\n",
    "There are five models in this notebook: DeepFM, NFM with both an adam optimizer and HB, and FM with an adam optimizer. Treatments for each models are referenced from [PyTorch Implementations of Factorization Machines](https://github.com/nzc/dnn_ctr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    NFMAdam(feature_sizes,\n",
    "            embedding_size=embedding_size,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            neuron_per_hidden_layer=neuron_per_hidden_layer,\n",
    "            n=n),\n",
    "    NFMOnn(feature_sizes,\n",
    "           embedding_size=embedding_size,\n",
    "           num_hidden_layers=num_hidden_layers,\n",
    "           neuron_per_hidden_layer=neuron_per_hidden_layer,\n",
    "           n=n),\n",
    "    DeepFMAdam(feature_sizes,\n",
    "               embedding_size=embedding_size,\n",
    "               num_hidden_layers=num_hidden_layers,\n",
    "               neuron_per_hidden_layer=neuron_per_hidden_layer,\n",
    "               n=n),\n",
    "    DeepFMOnn(feature_sizes,\n",
    "              embedding_size=embedding_size,\n",
    "              num_hidden_layers=num_hidden_layers,\n",
    "              neuron_per_hidden_layer=neuron_per_hidden_layer,\n",
    "              n=n),\n",
    "    FMAdam(feature_sizes,\n",
    "           embedding_size=embedding_size,\n",
    "           n=n)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = [str(model).split('-')[0] for model in model_list]\n",
    "print(model_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pretrain the models\n",
    "Models are pretrained with a batch of data from a dataset made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ith_model, ith_model_name in zip(model_list, model_name_list):\n",
    "    print(f\"====={ith_model_name}=====\")\n",
    "    for j in range(1000):\n",
    "        loss_emb = ith_model.update_embedding(batch_train_Xi_list[int(num_batch/2)],\n",
    "                                              batch_train_Xv_list[int(num_batch/2)],\n",
    "                                              batch_train_Y_list[int(num_batch/2)])\n",
    "        pred_label = ith_model.predict(batch_train_Xi_list[int(num_batch/2)],\n",
    "                                       batch_train_Xv_list[int(num_batch/2)])\n",
    "\n",
    "        if j % 100 == 0:\n",
    "            print('i th iter %d , loss : %f' % (j, loss_emb.cpu().data))\n",
    "            right_count = len((np.where(np.asarray(pred_label) == np.asarray(batch_train_Y_list[int(num_batch/2)])))[0])\n",
    "            total_count = len(np.asarray(batch_train_Y_list[int(num_batch/2)]))\n",
    "            print('training accuracy : %.4f\\n' % (right_count / total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fit models to the whole dataset\n",
    "After creating instances of models and pretraining them, we conduct the online task where embedding and weight parameters are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "result_dict['roc'] = {}\n",
    "result_dict['data_ratio'] = {}\n",
    "result_dict['time'] = {}\n",
    "result_dict['accuracy'] = {}\n",
    "\n",
    "result_dict['num_batch'] = num_batch\n",
    "result_dict['num_batchdata'] = num_batchdata\n",
    "result_dict['user_auc_mean'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ith_exp in range(num_batch):\n",
    "    print('#' * 100)\n",
    "\n",
    "    for jth_model_name, jth_model in zip(model_name_list, model_list):\n",
    "        print('%d th batch, %s model' % (ith_exp + 1, jth_model_name))\n",
    "        print('neg ratio : %d,  pos ratio %d ' % (ratio_list[ith_exp][0], ratio_list[ith_exp][1]))\n",
    "\n",
    "        time_elapsed, accuracy, roc, confusion_matrix\\\n",
    "            = jth_model.run_experiment(batch_train_Xi_list[ith_exp], batch_train_Xv_list[ith_exp], batch_train_Y_list[ith_exp])\n",
    "\n",
    "        print('fpr : %.4f , tpr : %.4f ' % (roc['fpr'], roc['tpr']))\n",
    "        print('confusion matrix : %s' % confusion_matrix)\n",
    "        print('accuracy : %.4f \\n' % accuracy)\n",
    "\n",
    "        if ith_exp == 0:\n",
    "            result_dict['roc'][jth_model_name] = [roc]\n",
    "            result_dict['data_ratio'][jth_model_name] = [ratio_list[ith_exp]]\n",
    "            result_dict['time'][jth_model_name] = [time_elapsed]\n",
    "            result_dict['accuracy'][jth_model_name] = [accuracy]\n",
    "\n",
    "        else:\n",
    "            result_dict['roc'][jth_model_name].append(roc)\n",
    "            result_dict['data_ratio'][jth_model_name].append(ratio_list[ith_exp])\n",
    "            result_dict['time'][jth_model_name].append(time_elapsed)\n",
    "            result_dict['accuracy'][jth_model_name].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save models\n",
    "When the training is done, models are saved to a designated directory as pickle files. Their ROC scores, accuracy scores, and amount of time required are also saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename = 'Time_Stamp' + str(int(time()))\\\n",
    "                + '-Dataset' + str('criteo') \\\n",
    "                + '-Num_BatchLength' + str(num_batchdata) \\\n",
    "                + '-Num_Batch' + str(num_batch) \\\n",
    "                + '-Num_Hidden_Layers' + str(num_hidden_layers) \\\n",
    "                + '-Neuron_Per_Hidden_Layer' + str(neuron_per_hidden_layer) \\\n",
    "                + '_' + str(data_config)\n",
    "\n",
    "with open(save_log + save_filename + '.pickle', 'wb') as f:\n",
    "    pickle.dump(result_dict, f)\n",
    "\n",
    "for ith_model, ith_model_name in zip(model_list, model_name_list):\n",
    "    with open(save_model + str(ith_model_name) + '.pickle', 'wb') as f:\n",
    "        pickle.dump(ith_model, f)\n",
    "\n",
    "print('save_log : %s' % (save_log + save_filename + '.pickle'))\n",
    "print('save_model : %s' % (save_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot graphs\n",
    "We are able to draw ROC curves and accuracy score graphs according to the models' performance. For the ROC curve, the x-axis indicates false positive rates (FPR or (1-specificity)) and the y-axis indicates true positive rates (TPR or sensitivity). The x-axis and the y-axis in the accuracy score graph indicates the sequence of iteration and number of correct answers that the model has made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot.draw_roc_graph(\"../performance/save_log/\", \"Time_Stamp1578375578-Datasetcriteo-Num_BatchLength2500-Num_Batch10-Num_Hidden_Layers5-Neuron_Per_Hidden_Layer10_7\" + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.draw_acc_graph(\"../performance/save_log/\", \"Time_Stamp1578375578-Datasetcriteo-Num_BatchLength2500-Num_Batch10-Num_Hidden_Layers5-Neuron_Per_Hidden_Layer10_7\" + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}